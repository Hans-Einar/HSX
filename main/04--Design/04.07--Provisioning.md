# Design Spec - Provisioning & Persistence
# 04.07--Provisioning Design (v3, repo style)
**Status:** DRAFT | **Date:** 2025-10-28 | **Owner:** HSX Core

> **Design stance:** Provisioning is an executive-owned orchestration service. It performs no autonomous image selection, embeds no transport logic, and coordinates all loading through the MiniVM control-plane API. All policy decisions (what to load, where to persist) are driven externally by the executive.

**Authoritative context (repo-local):**
- Architecture: [03.07--Provisioning](../03--Architecture/03.07--Provisioning.md), [03.00--Architecture](../03--Architecture/03.00--Architecture.md)
- Study & Requirements: [02--Study](../02--Study/02--Study.md), [02.01--Requirements](../02--Study/02.01--Requirements.md)
- Implementation-phase specs (normative inputs referenced by design):
  [docs/hxe_format.md](../../docs/hxe_format.md), [docs/executive_protocol.md](../../docs/executive_protocol.md),
  [docs/resource_budgets.md](../../docs/resource_budgets.md), [04.01--VM.md](04.01--VM.md)

**Traceability (fill from 02.01--Requirements):** Refs DR[..], DG[..], DO[..].

## 1. Scope
- Executive-owned provisioning service for HXE image loading via monolithic and streaming modes (DR-1.1, DR-3.1, DR-5.1, DR-5.3).
- CAN broadcast, SD manifest, and host file provisioning workflows with integrity validation (DR-1.1, DG-5.3).
- FRAM/flash persistence layer for calibration data and state retention tied to value subsystem (DR-5.3, DG-7.3).
- Progress reporting and telemetry via event stream with back-pressure handling (DG-5.2, DG-8.1).

## 2. Preconditions
- Executive services for mailbox/value/persistence available; CAN/SD transport drivers operational (DR-1.1, DR-5.3, DR-6.1, DR-7.1, DG-5.3).
- Toolchain supplies .hxe images, manifests, and integrity metadata (DR-3.1, DR-5.3, DG-3.1, DG-3.5).
- Hardware platform provisions FRAM/flash space per docs/resource_budgets.md (DR-5.2, DR-5.3).

## Postconditions
- Provisioning workflows (CAN broadcast, SD manifest, host load) reliably install/update HSX apps with integrity checks (DR-1.1, DR-3.1, DR-5.1, DR-5.3).
- Persistence layer (FRAM) stores calibration/state tied to value subsystem policies (DR-5.3, DG-7.3).
- Telemetry/CLI commands report provisioning status and recovery options (DG-5.2, DG-8.1).

## Scope
- Node startup sequence, image loading, task lifecycle (DR-1.1, DG-1.2).
- CAN and SD provisioning protocols (DR-1.1, DG-5.3).
- FRAM persistence flow for calibration/state (DR-5.3, DG-7.3).
- Reference `.hxe` header + manifest schema in `docs/hxe_format.md` for loaders (DR-3.1, DR-5.3, DG-3.5).
- CAN and SD provisioning protocols (DR-1.1, DG-5.3).
- FRAM persistence flow for calibration/state (DR-5.3, DG-7.3).

# Provisioning & HXE Loading (Monolithic + Streaming)

## Intent
The Executive is responsible for delivering HSX application images (HXE) to MiniVM, validating them, persisting them when required, and coordinating the transition from “loading” to “ready” tasks. The design supports both monolithic loads (entire HXE already resident in host memory) and streaming loads (byte-by-byte ingestion via HAL transports such as filesystem, CAN, or UART). MiniVM remains transport- and filesystem-agnostic and exposes only a narrow “probe + execute” surface; all orchestration, policy, and persistence live here.

## Responsibilities (Executive)
- **Source selection and transport:** choose and drive a HAL binding (FS, CAN, UART, …). Track source details (filepath or CAN master node) for ps command reporting.
- **Image verification:** header/section checks including app_name extraction and allow_multiple_instances flag, optional signature and capability checks, CRC.
- **Metadata preprocessing (HXE v2):** Before VM load:
  - Parse metadata section table from HXE header (`meta_offset`, `meta_count`).
  - Process `.value` section: register all values with app's PID, build descriptors.
  - Process `.cmd` section: register all commands with app's PID, store handler offsets.
  - Process `.mailbox` section: create mailboxes for the app.
  - **Strip metadata sections** from image before passing to VM (or mark for VM to ignore).
  - This eliminates all registration overhead from VM execution.
- **App name conflict detection:** When loading HXE, check if app_name already exists in task list:
  - If `allow_multiple_instances` flag is **not set** and app_name exists: reject load with `EEXIST`.
  - If `allow_multiple_instances` flag **is set** and app_name exists: append `_#0`, `_#1`, etc. to create unique instance name.
- **Coordination with VM:** call `vm_load_begin/write/end/abort` (streaming) or `vm_load_hxe` (monolithic) with code/rodata/bss only (metadata already processed); call `vm_set_context` as needed; never force VM to auto-switch.
- **Persistence and rollback:** write HXE and/or parameter blocks to FRAM/flash when policy requires; provide atomic swap and rollback on failure.
- **Progress/events:** emit structured progress and status events with back-pressure/ACK; support cancellation and retries.
- **Policy & security:** enforce who may load, where it may be stored, and whether signatures/capability gates are required.

## Interfaces (Executive-side)

### A) Host-Facing Control
_Examples; actual names may live in the tooling API_
- `provision.load.from_file(path, opts) → pid, rc`
- `provision.load.from_can(channel, meta, opts) → pid, rc`
- `provision.status(pid) → { state ∈ { IDLE, LOADING, VERIFY, READY, FAILED }, progress%, last_error }`
- `provision.abort(pid) → rc`
- `provision.persist(pid, target=FRAM|FLASH) → rc`
- `provision.activate(pid) → rc`
- `provision.rollback(last_ok_rev) → rc`

### B) Executive ↔ VM
_Uses VM control-plane; see MiniVM design_
- `vm_load_hxe(hxe_ptr, len) → pid, rc` _(monolithic)_
- `vm_load_begin() → pid, rc`
- `vm_load_write(pid, chunk*, n) → rc`
- `vm_load_end(pid) → rc`
- `vm_load_abort(pid) → rc`
- `vm_set_context(pid) → rc`

### C) Executive ↔ HAL
_Illustrative bindings_
- `hal.fs.open/read/close(path)`
- `hal.can.recv(stream_id, timeout)` / `hal.can.send(…)`
- `hal.uart.read(n, timeout)`
- `hal.fram.{read,write,commit,rollback}`

HAL APIs are synchronous or callback/async; Executive owns buffering and retries.

## Loading Modes

### Monolithic (`vm_load_hxe`)
Use when the full HXE image is already in host memory (e.g., read from file). The Executive performs a quick precheck (optionally including signature/capability checks) and then calls `vm_load_hxe`. VM allocates arenas, creates a PID, maps code/rodata, prepares `.data/.bss`, initialises WP, and returns `pid` on success. Executive then transitions the PID to **READY** and may optionally persist metadata or the image to FRAM/flash according to policy.

### Streaming (`vm_load_begin/write/end/abort`)
Use when ingesting over constrained transports (CAN/UART) or on low-RAM targets.

- **Begin:** Executive calls `vm_load_begin() → pid`; VM allocates arenas and sets PID state to **LOADING**.
- **Write loop:** Executive reads chunks from HAL and calls `vm_load_write(pid, chunk, n)`. MiniVM performs early header/section checks and accumulates CRC; invalid bytes are rejected immediately, avoiding large host buffers.
- **End:** Executive calls `vm_load_end(pid)`. VM performs final header/section validation and CRC; on success, PID transitions to **READY**.
- **Abort:** On any failure or timeout, Executive calls `vm_load_abort(pid)` and emits an error event; VM frees arenas and invalidates PID.

## State Machine (Executive view)

```
IDLE → (request load) → LOADING → VERIFY → READY
   ↘ on vm_load_write error / vm_load_end failure → FAILED → (optional rollback/purge) → IDLE
   ↘ on provision.abort → ABORTING → IDLE
```

## Error Model and Mapping

**Representative VM-level errors:**
- `EBADMSG` — malformed header/section or CRC mismatch (early or final)
- `ENOSPC` — insufficient memory for arenas
- `ESRCH` — unknown or non-LOADING PID for write/end/abort
- `EBUSY` — invalid state transition (e.g., write after end)

**Executive maps these to host/tooling responses and events:**
- `provisioning.error { pid, code, where: begin|write|end|verify, detail }`
- `provisioning.progress { pid, bytes_written, total?, phase }` _(rate-limited)_
- `provisioning.complete { pid, ready=true }`
- `provisioning.aborted { pid }`

On failure with persistence enabled, Executive performs rollback (see **Persistence**).

## Policy & Security
- **Access control:** only authorised sessions may initiate provisioning; policy may restrict target (e.g., allow only test device).
- **Signature & capabilities (optional):** verify signature, version, and HXE capability flags (e.g., `requires_f16`, `needs_mailbox=N`) prior to `vm_load_*`.
- **Compatibility gates:** ensure HXE ABI/SVC version is acceptable (`EXEC_GET_VERSION`).
- **Resource budgets:** check code/data/stack sizes against per-target limits before allocation.

## Persistence & Rollback
- **FRAM/flash schema:** store either the HXE blob, a manifest pointer, and per-task metadata (PID, stack size, optional heap size, version/capability flags).
- **Commit protocol:** write new image to staging; verify CRC; atomically swap active pointer; emit `provisioning.persisted`.
- **Rollback:** on verification failure or boot fault, revert to prior known-good pointer; emit `provisioning.rollback`.
- **Wear management:** track write counts; throttle or rotate partitions per `resource_budgets`.

## HXE Header Hints (Stack/Heap)
If present, the Executive reads optional header hints before allocation:
- `stack_size_bytes` — requested stack arena size; subject to policy/limits.
- `heap_size_bytes` — only relevant if runtime/app uses new/malloc; may be ignored when heap not enabled.

These are passed through to VM (arena sizing) or used for policy decisions; absence implies defaults.

## Progress & Events (with Back-Pressure)
Executive must emit explicit, rate-limited progress events during streaming; clients must ACK or the Executive will coalesce/shed events according to `executive_protocol`.

**Minimum events:** `provisioning.started`, `provisioning.progress`, `provisioning.complete`, `provisioning.error`, `provisioning.aborted`, `provisioning.persisted`, `provisioning.rollback`.

All events include `pid`, `phase`, and `ts`; progress includes `bytes_written` and, when available, `total_bytes`.

## Timeouts & Retries
For streaming over lossy links, the Executive maintains per-PID timers:
- `write_timeout`: maximum inter-chunk gap before abort;
- `total_timeout`: maximum wall-clock for the session;
- **retry policy:** resend or re-request chunk (transport-specific).

On timeout, emit `provisioning.error/aborted`, call `vm_load_abort`, and reclaim resources.

## Concurrency & Locking
- One active streaming session per PID.
- Multiple concurrent streams permitted if resource budgets allow; each PID isolates its arenas.
- Provisioning is orthogonal to debug sessions; if a PID is under debug, provisioning must block or require explicit release.

## Example Sequences

### A) Monolithic file → READY
1. **host:** `open(path) → read all → verify signature (optional)`
2. **exec:** `vm_load_hxe(buf, len) → pid`
3. **exec:** `vm_set_context(pid)` _(optional)_
4. **exec:** emit `provisioning.complete{pid}`

### B) Streaming over CAN
1. **exec:** `vm_load_begin() → pid`; emit `provisioning.started{pid}`
2. **loop:** `hal.can.recv(chunk)` → **exec:** `vm_load_write(pid, chunk, n)`; emit `provisioning.progress` _(rate-limited)_
3. **exec:** `vm_load_end(pid)` → emit `provisioning.complete{pid}`
4. _(optional)_ **exec:** `provision.persist(pid, FRAM)` → emit `provisioning.persisted{pid}`

### C) Failure during streaming
- `vm_load_write(pid, chunk) → EBADMSG`
- **exec:** `provisioning.error{pid, EBADMSG, where=write}` → `vm_load_abort(pid)` → reclamation → `provisioning.aborted{pid}`

## Testing & DoD (hooks → 06–Test)
- Happy-path monolithic/streaming loads (FS and CAN mocks).
- Early-reject cases (bad magic, bad section table, CRC mismatch).
- Abort paths (transport timeout, user abort) free all reservations.
- Persistence/rollback simulation with power-fail injection between write and commit.
- Event rate-limit/ACK behaviour and progress monotonicity.
- Resource budget enforcement (oversized code/data/stack rejected pre-allocation).

**Done-when:**
- All API flows implemented and covered by tests above.
- Events conform to `executive_protocol` (fields, ACK, rate-limit).
- VM loader contract (monolithic + streaming) passes golden HXE fixtures (good/bad/edge).
- Persistence/rollback validated against FRAM/flash mock with wear accounting.
- Documentation links resolved to `abi_syscalls`, `hxe_format`, `resource_budgets`, `executive_protocol`.



## Data Structures
- - **Provisioning manifest:** per-image metadata (target PID, required capabilities, FRAM keys, default values) defined in `docs/hxe_format.md`.
- **Transfer session state:** for CAN/SD, track sequence numbers, payload chunks, CRC progress, timeout timers.
- **FRAM table:** mapping of OID → FRAM address, mode (volatile/load/save), checksum.
- **Boot configuration:** per-node startup script indicating which `.hxe` files to load, priority ordering, fallbacks.

## Algorithms / Behaviour
- **Boot sequence:**
  1. Bootloader loads host firmware; executive initialises HAL.
  2. Executive scans provisioning sources in priority order (host preload → SD manifest → CAN master).
  3. For each `.hxe` entry, verify header/CRC, allocate PID, create task via MiniVM, restore persisted values, start execution.
- **Runtime update:** when new image arrives (CAN/host CLI), load into staging, validate, stop existing PID, replace while preserving `val.persist` data, emit progress events via mailbox.
- **CAN protocol:** use `GET`, `SET`, `LOAD` opcodes (11-bit CAN ID structure), support chunked transfers with acknowledgements and retry/back-off.
- **SD manifest:** simple TOML/JSON listing image paths, version, FRAM manifest references; parsed at boot.
- **FRAM persistence:** integrate with value subsystem; writes debounced to minimise wear. On failure (CRC, write error) roll back and notify host.

## Edge Cases
- Partial downloads or CRC failure recovery.
- Version mismatches between app and host firmware.
- Power loss during persistence update.
- Duplicate PID request → executive reuses or reallocates depending on manifest policy.
- Missing FRAM key referenced by manifest -> load fails with diagnostics; fallback to defaults.
- Security/policy (future): authenticate CAN payloads, restrict commands; documented in `docs/security.md` once defined.

## Testing Considerations
- Simulated CAN/SD provisioning scenarios.
- Persistence regression tests (power cycle simulations).
- Integration tests covering calibration updates and rollback.
- Fuzz hostile CAN traffic (out-of-order chunks, malformed headers) to ensure robust recovery.
- Validate persistence handshake: load values, update via `val.set`, power cycle simulation verifying restoration.

## Traceability
- **Design Requirements:** DR-1.1, DR-1.2, DR-3.1, DR-5.1, DR-5.2, DR-5.3, DR-6.1, DR-7.1.
- **Design Goals:** DG-1.2, DG-3.1, DG-3.5, DG-5.1�5.3, DG-7.3.
- **Design Options:** DO-relay (future relay considerations for provisioning status).


