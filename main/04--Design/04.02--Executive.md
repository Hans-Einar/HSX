# Design Spec - Executive

## 1. Scope
- Host controller managing MiniVM lifecycle, scheduling, and IPC services (DR-5.1, DR-5.3, DG-5.1).
- Protocol server (JSON-over-TCP, UART, or direct API) consumed by shell, debugger, and tooling clients (DR-8.1, DG-5.2, DG-8.1).
- Trace and debug support with configurable buffer sizes for different executive variants (minimal, development, full debugger) (DR-8.1, DG-5.2).
- **HXE metadata preprocessing:** Parse and register values/commands/mailboxes from `.value`/`.cmd`/`.mailbox` sections (HXE v2) before VM execution (DR-3.1, DR-7.1, DG-7.1).
- Provisioning workflows that load and reload tasks, manage FRAM persistence, and serve CAN, SD, or host sources (DR-1.1, DR-5.3, DG-1.2).
- Debug traps (BRK, single-step) and SVC invocations handled synchronously through step results (DR-8.1, DG-5.2).

### 1.1 Modular Executive Architecture
The executive is built from pluggable modules that can be compiled/linked together to create different executive variants:

**Core modules (always present):**
- VM lifecycle management (load, step, context switching)
- SVC dispatcher and syscall handlers
- Basic scheduler (single-instruction quanta, task states)

**Backend modules (configurable):**
- **Filesystem backends:** Host filesystem (Python/desktop), SPI SD card (MCU), CAN provisioning
- **Protocol/shell backends:** JSON-RPC over TCP (Python), direct C API (MCU), UART command interface
- **HAL implementations:** Platform-specific drivers for UART, CAN, FRAM, GPIO, etc.

**Debug/trace modules (optional):**
- **Minimal:** No trace storage, VM keeps only last instruction for basic status
- **Development:** Trace buffer (~100 instructions) for post-mortem debugging
- **Full debugger:** Large trace buffer (1000s of instructions), breakpoint management, event streaming, symbol lookup

**Executive variants:**
- **MCU minimal:** Core + minimal trace + UART shell + SPI SD backend (~10-15 KiB flash)
- **MCU production:** Core + development trace + CAN provisioning + FRAM persistence (~20-30 KiB flash)
- **Desktop development:** Full debugger + JSON-RPC + host filesystem + event streaming (Python)

The modular design allows selecting only required features at compile/link time, minimizing flash and RAM usage on constrained MCU targets while supporting full debugging on desktop.

### 1.2 Operating Principles
- Drive the VM strictly via attached stepping (`vm_set_context`, `vm_step`, `vm_clock`); no autonomous scheduling runs inside the VM.
- **Preprocess HXE metadata sections:** Before VM starts, parse `.value`/`.cmd`/`.mailbox` sections and register all resources with app's PID, eliminating VM initialization overhead.
- Provide an SVC bridge that dispatches module requests (mailbox, value, command, provisioning, HAL shims) to host handlers.
- Manage debugger sessions, breakpoint lists, and event stream delivery with acknowledgement and rate limiting (full debugger variant only).
- Expose a host control plane for tooling (attach, step/clock, breakpoint management, register access, process status).

## 2. Preconditions
- MiniVM provides synchronous step interface with status returns (reason: ok/break/fault/svc) and honours SVC/BRK semantics as described in DG-5 and DG-6 (DR-8.1, DG-4.2, DG-5.2).
- MiniVM exposes minimal trace state (last instruction) that executive can poll after each step for debug/trace purposes.
- Mailbox, value/command, and provisioning services exist (per architecture sections 3.3 and 3.4) so executive callbacks can delegate work (DR-6.1, DR-7.1, DR-5.3).
- Toolchain provides `.hxe` images and, when available, symbol metadata that enables stack reconstruction and disassembly annotations (DR-3.1, DG-3.3, DR-2.5).
- Host environment exposes networking or transport required for the debugger CLI/TUI (TCP loopback on desktop, UART relay on embedded targets) (DR-8.1, DG-8.3).

## 3. Postconditions
- Scheduler enforces single-instruction quanta with PID locking and exposes state required by debugger tooling (DR-5.1, DG-5.4, DG-5.3).
- Session and event RPCs (`session.open`, `events.subscribe`, etc.) stream structured events with documented back-pressure controls (DR-8.1, DG-5.2, DG-8.2).
- PID-level services (`ps`, `stack`, `memory`, `watch`) provide the data expected by the HSX debugger/TUI (`functionality/#1_TUI_DEBUGGER`) (DR-8.1, DG-8.1).
- Provisioning, stdio, mailbox, and persistence hooks remain backwards compatible while exposing the new debugger instrumentation (DR-1.1, DR-5.3, DR-6.1, DR-7.1).

## 4. State & Data Model
### 4.1 ExecutiveState
- `tasks: Dict[int, TaskRecord]` stores PID metadata and scheduling state.
- `current_pid` tracks which task is currently active in the VM.
- `waiting_tasks`, `auto_thread`, `auto_event`, `clock_mode` hold scheduler control.
- Logging and accounting (`_next_log_seq`, `log_buffer`, `scheduler_trace`, `scheduler_counters`) capture diagnostics.
- `sessions: Dict[str, Session]` records active client sessions and their locks.

**Note on context state:** The executive does NOT store or manage task context (PC, SP, registers). Context state lives in the VM and is managed via `vm_set_context(pid)`. This ensures security and proper encapsulation—the executive schedules PIDs but never directly manipulates execution state.

Runtime enforcement mirrors this contract: the Python reference executive exposes dedicated register access RPCs (`vm_reg_get`, `vm_reg_set`, and their `*_for` variants) so debugger features never poke into cached task snapshots. The scheduler asserts that READY/RUNNING/WAIT_MBX/SLEEPING tasks report non-zero `reg_base` / `stack_base` metadata and refuses to persist per-task register arrays. Violations are logged and surfaced as assertion failures, tightening the feedback loop for the remediation tracked under [issues/#2_scheduler](../../issues/#2_scheduler/hsx_scheduler_implementation_review.md).

### 4.2 TaskRecord
```python
{
    "pid": int,
    "state": str,            # running, ready, waiting_mbx, paused, returned, terminated
    "priority": int,
    "quantum": int,
    "program": str,
    "app_name": str,         # Application name from HXE header
    "filepath": str,         # Source path (file path or CAN master node details)
    "stdout": str,
    "sleep_pending": bool,
    "sleep_deadline": Optional[float],
    "trace": bool,
    "exit_status": Optional[int],
}
```

**VM-internal state:** Fields like `pc`, `sp`, `reg_base`, `stack_base`, `stack_limit` are managed internally by the VM and not stored in the executive's TaskRecord. The executive only needs to know the PID and scheduling state. When the executive wants to inspect or modify these for debugging, it uses VM APIs like `vm_reg_get(pid, reg_id)` which are policy-gated.

**App name and multiple instances:**
- `app_name` is extracted from the HXE header during loading.
- If HXE header allows multiple instances, and app_name already exists, the executive appends `_#0`, `_#1`, etc. to create unique instance names.
- If HXE header disallows multiple instances and app_name already exists, loading fails with `EEXIST` error.
- `filepath` stores the source: file path for host loads, or CAN master node details (e.g., `CAN:node_id:channel`) for CAN-loaded apps.

### 4.3 Sessions, Events, and Provisioning
- Session records (`Session(id, client_info, lock_pid, capabilities, keepalive_ts, event_cursor, filters)`) guarantee at most one debugger per PID while allowing observer sessions.
- Event bus: bounded ring buffer of `Event{seq, ts, type, pid, payload}` with per-session cursors. Owner sessions retain priority; observer cursors are evicted first when retention limits trigger.
- Provisioning: see [04.07--Provisioning.md](04.07--Provisioning.md)
- Resource budgets (per target, TBD) capture limits for PID count, register arenas, stack pools, mailbox descriptors, and FRAM capacity. Final numbers live in `docs/resource_budgets.md` and feed scheduler/resource guardrails.

## 5. Interfaces
### 5.1 SVC / ABI (module 0x06 EXEC)
> Authoritative definitions live in [abi_syscalls.md](../../docs/abi_syscalls.md).

Module 0x06 provides executive-level syscalls for operations that don't fit naturally in other modules:

| Func | Mnemonic | R0 (input) | R1 | R2 | R3 | R0 (return) | Status |
|------|----------|------------|----|----|----|----|--------|
| 0x00 | EXEC_SLEEP_MS | sleep_duration_ms | - | - | - | 0 | Implemented |

Applications should not need explicit yield or scheduling syscalls—context switching happens automatically when tasks block on mailbox operations or sleep. The executive manages task scheduling transparently.

### 5.2 Host Control-Plane APIs (non-SVC)
| API | Args | Returns | Notes |
|-----|------|---------|-------|
| `session.open(pid?)` | PID or `null` | rc | Claims exclusive debug session (locks PID set). |
| `session.close()` | - | rc | Releases session lock and subscriptions. |
| `ps()` | - | list[pid, app_name, state, filepath] | Snapshot of tasks with app name and source path. |
| `vm.set_context(pid)` | pid | rc | Selects active PID before register/memory ops. |
| `vm.step()` / `vm.clock(n)` | - / n | StepResult | Drives VM under scheduler rules. |
| `reg.get(pid, reg_id)` | pid, const | u32 | Reads register via VM helpers. |
| `reg.set(pid, reg_id, value)` | pid, const, u32 | rc | Writes register (policy gated). |
| `bp.set(pid, addr)` | pid, code addr | rc | Adds breakpoint to per-PID set. |
| `bp.clear(pid, addr)` | pid, code addr | rc | Removes breakpoint. |
| `bp.list(pid)` | pid | list[addr] | Lists breakpoints. |
| `stack.info(pid, max_frames)` | pid, frame_limit | list[frame] | Reconstructs call stack (see Section 5.3). |
| `disasm.read(pid, addr, count, mode)` | pid, addr, count, mode | list[inst] | Returns disassembled instructions with symbols (see Section 5.4). |
| `symbols.list(pid, type)` | pid, type_filter | list[symbol] | Enumerates functions/variables from loaded .sym file (see Section 5.5). |
| `memory.regions(pid)` | pid | list[region] | Returns memory map (code/data/stack/heap regions) (see Section 5.6). |
| `watch.add(pid, expr)` | pid, symbol/addr | watch_id | Creates watch on variable or address (see Section 5.7). |
| `watch.remove(watch_id)` | watch_id | rc | Removes watch. |
| `watch.list(pid)` | pid | list[watch] | Lists active watches. |

Tool breakpoints remain executive-side comparisons; no code patching inside the guest is required.

### 5.3 Stack Reconstruction API
The executive walks the call stack using saved frame pointers and return addresses:

**Algorithm:**
1. Start with current PC, SP, FP from task context
2. Look up symbol for PC in loaded .sym file
3. Read saved return address and previous FP from stack
4. Repeat until FP == 0 or stack limit reached

**Implementation:**
```python
def reconstruct_stack(pid: int, max_frames: int = 32) -> List[StackFrame]:
    frames = []
    current_pc = vm_reg_get(pid, REG_PC)
    current_sp = vm_reg_get(pid, REG_SP)
    current_fp = vm_reg_get(pid, REG_FP)
    
    for depth in range(max_frames):
        symbol = symbol_lookup(pid, current_pc)  # From loaded .sym
        frames.append({
            "depth": depth,
            "pc": current_pc,
            "sp": current_sp,
            "fp": current_fp,
            "symbol": symbol.get("name") if symbol else None,
            "file": symbol.get("file") if symbol else None,
            "line": symbol.get("line") if symbol else None
        })
        
        if current_fp == 0 or current_fp >= stack_limit:
            break
        
        return_addr = vm_mem_read(pid, current_fp - 4, 4)
        prev_fp = vm_mem_read(pid, current_fp, 4)
        
        if return_addr == 0:
            break
        
        current_pc = return_addr
        current_sp = current_fp
        current_fp = prev_fp
    
    return frames
```

### 5.4 Disassembly API
The executive provides on-demand disassembly with symbol annotations:

**Symbol Loading:**
- Executive loads `.sym` JSON file alongside `.hxe` on task load (if available)
- Caches symbol table, line mappings, and function boundaries
- Falls back to on-the-fly disassembly without symbols if .sym unavailable

**Modes:**
- `from_addr`: Returns `count` instructions starting at `addr`
- `around_pc`: Returns `count/2` instructions before and after current PC

**Caching Strategy:**
- Cache full disassembly on first request (desktop executive variant)
- On-demand per-instruction decode (MCU executive variant)
- Symbols remain cached for task lifetime

### 5.5 Symbol Enumeration API
The executive exposes symbols loaded from `.sym` files:

**Symbol Types:**
- `function`: Function/subprogram symbols
- `variable`: Global and static variables
- `all`: All symbols

**Symbol Table Structure:**
```python
{
    "functions": [
        {"name": "main", "address": 0x0100, "size": 64, "file": "main.c", "line": 10}
    ],
    "variables": [
        {"name": "counter", "address": 0x2000, "size": 4, "type": "uint32_t", "scope": "global"}
    ]
}
```

### 5.6 Memory Region Info API
The executive reports memory layout from loaded .sym file or HXE header:

**Region Types:**
- `text`: Code section (executable)
- `data`: Initialized data
- `bss`: Uninitialized data
- `stack`: Stack region
- `heap`: Heap region (if applicable)

**Source:**
- Memory regions from .sym file (if available)
- Fallback to HXE header (code_len, ro_len, bss_size)

### 5.7 Watch Expression Evaluation
The executive tracks watched memory locations and emits events on changes:

**Watch Mechanism:**
1. Client issues `watch.add` with symbol name or address
2. Executive resolves symbol to memory address using loaded .sym
3. Executive stores watch descriptor: `{watch_id, pid, addr, size, last_value}`
4. On each `trace_step` event, executive checks watched addresses
5. If value changed, emit `watch_update` event

**Watch Descriptor:**
```python
{
    "watch_id": 1,
    "pid": 2,
    "expr": "counter",
    "addr": 0x2000,
    "size": 4,
    "type": "uint32_t",
    "last_value": 0x00000005
}
```

**Performance:**
- Check all watches after each step (acceptable for <100 watches)
- Optimize: Track dirty memory pages, check only affected watches

## 6. Operational Behaviour
### 6.1 RPC Handling
1. Parse JSON payload and validate the protocol `version` (currently 1).
2. Retrieve and verify the session, enforcing lock semantics (one debugger per PID set).
3. Dispatch to the handler (load, attach, clock, scheduler, mailbox, value/command, provisioning).
4. Convert exceptions to `{"status": "error", "error": ...}` and append diagnostics to `log_buffer`.

### 6.2 Executive-VM Relationship
The VM always starts with an executive attached—there is no production standalone mode. The executive is responsible for:
- Loading HXE code into the VM
- Driving execution via `step()` or `clock()` calls
- Handling all syscall traps
- Managing multi-task scheduling

**Modular executive design:**
The executive consists of pluggable backend modules that can be configured at compile/link time:
- **Filesystem backends:** Host filesystem (Python/desktop), SPI-connected SD card (MCU), or CAN-based provisioning
- **Shell/protocol backends:** TCP/IP + JSON-RPC (Python), direct API calls via headers (C on MCU), or UART command interface
- **Hardware abstraction:** Different HAL implementations for desktop simulation vs. embedded targets

On MCU targets, the VM and executive are compiled and linked together into a single flash image with the appropriate backend modules selected.

**Legacy attach/detach hooks:**
While the current design assumes the executive is always present, attach/detach mechanisms are preserved in the API for potential future scenarios (e.g., hot-swapping executives or upgrading from a minimal boot executive to a full-featured one). For now, `MiniVM.attached` should always be `True` in production.

### 6.3 Clock Modes
The executive controls VM execution through explicit stepping:
- **`manual`**: Only explicit `step` or `clock` RPCs advance execution. Used for debugging and single-stepping.
- **`auto`**: Background thread issues batched steps at a configured rate. Used for normal execution when no debugger is attached.

The executive always drives the clock—there is no "detached" mode where the VM runs autonomously.

### 6.4 Trace and Debug Support
**VM trace state (minimal):**
The VM maintains only the last executed instruction snapshot:
- `last_pc`: PC before last instruction execution
- `last_opcode`: Full instruction word
- `last_regs`: Register snapshot before execution (optional, debug builds only)

After each `vm_step()`, the executive reads this minimal state if trace is enabled.

**Executive trace storage (configurable):**
Trace buffers live in the executive, sized according to variant:
- **Minimal executive:** No trace buffer—only last instruction from VM for status
- **Development executive:** Circular buffer of ~100 trace records for post-mortem debugging
- **Full debugger executive:** Large buffer (1000+ records) plus call stack reconstruction

**Trace record format:**
```python
{
    "seq": int,           # Monotonic sequence
    "pid": int,           # Task that executed
    "pc": int,            # PC before execution
    "opcode": int,        # Full instruction word
    "regs": [16],         # Register snapshot (optional)
    "mem_access": {...}   # For LD/ST instructions (optional)
}
```

**Benefits of executive-side trace:**
- VM stays simple—only captures last instruction
- Executive controls buffer size based on variant
- MCU builds can omit trace entirely to save RAM
- Desktop builds can have deep trace for debugging
- No async event complexity—executive polls VM state after step

**Event streaming (full debugger only):**
Full debugger variant may provide event streaming to debugger clients:
- Collect trace records, breakpoint hits, scheduler transitions
- Stream via protocol backend (JSON-RPC, etc.)
- Implement back-pressure and ACK protocol for subscribers
- Minimal/development variants omit this entirely

### 6.5 Provisioning Flow
1. Host CLI issues `load <path>` or a CAN/SD transfer triggers provisioning.
2. Executive verifies the `.hxe` header, allocates a PID, and prepares register and stack arenas.
3. If FRAM keys exist, persisted values load before the task resumes.
4. CAN/SD transfers report progress and errors through mailbox or value channels back to the host.

### 6.6 Persistence Hooks
- `val.persist` binds values to FRAM keys. The executive loads persisted values on task start and writes updates on `val.set` (with debounce) or on shutdown. FRAM layout guidance lives in `docs/resource_budgets.md` (P0).

### 6.7 StdIO Routing
- On task load the executive binds `svc:stdio.*` mailboxes. Shell `listen` commands attach to these descriptors for stdout and stderr streaming.

### 6.8 Resource Envelopes
- Honor limits from `docs/resource_budgets.md`: <=28 KiB text, <=5.5 KiB SRAM for runtime, plus per-task stack and register allocations when mapping C structures and linker scripts.

### 6.9 Debugger Session Handshake
- `session.open` negotiates protocol version and capabilities, acquires optional PID locks, and returns heartbeat interval plus maximum event buffer size.
- `session.keepalive` refreshes idle timers. `session.close` releases locks and unsubscribes from all event feeds.

## 7. Event Streaming Contract
> Keep `docs/executive_protocol.md` aligned with this section; new event types, fields, or RPCs must be documented there.

### 7.1 Subscribe Flow
1. Client issues `session.open` with fields such as `client` (identifier), `version` (protocol integer), `capabilities` (`{"features": ["events", "stack", "watch"], "max_events": 256}`), and optional `pid_lock` (int or `null`).
2. Executive responds with `session_id`, negotiated capability set, heartbeat interval, and warnings for unsupported features.
3. Client sends `events.subscribe`:

```json
{
  "version": 1,
  "cmd": "events.subscribe",
  "session": "<id>",
  "filters": {
    "pid": [1, 3],
    "categories": ["debug_break", "trace_step", "scheduler", "mailbox", "watch"],
    "since_seq": null
  }
}
```

4. Executive streams newline-delimited JSON events until `events.unsubscribe`, session close, or disconnect.

### 7.2 Event Schema
```json
{
  "seq": 1024,
  "ts": 1739730951.512,
  "type": "debug_break",
  "pid": 2,
  "data": {
    "pc": 4096,
    "symbol": "main.loop",
    "reason": "BRK"
  }
}
```
- Required fields: `seq` (monotonic uint64), `ts` (float seconds since epoch), `type` (string), `pid` (int or `null`), `data` (object).
- Canonical types include:
  - `trace_step` (`pc`, `opcode`, `flags`, optional `changed_regs`): Single instruction execution with optional register change tracking
  - `debug_break` (`pc`, `reason`, `breakpoint_id`): Breakpoint hit or BRK instruction
  - `task_state` (`prev_state`, `new_state`, `reason`): Task state transition (running/paused/stopped/etc.)
  - `scheduler` (`state`, `prev_pid`, `next_pid`, `reason`, `quantum_remaining`, `post_state`, `next_state`, `executed`, `source`): Scheduler context switch
  - `mailbox_send` / `mailbox_recv` (`descriptor`, `length`, `flags`): Mailbox operations
  - `watch_update` (`watch_id`, `expr`, `old_value`, `new_value`, `formatted`): Watched variable changed
  - `stdout` / `stderr` (`text`): Stdio output
  - `warning` (`message`, `category`): System warning
- Clients must ignore unknown fields for forward compatibility.

`scheduler` events are emitted for every context hand-off. `reason` is restricted to `quantum_expired`, `sleep`, `wait_mbx`, `paused`, or `killed` so downstream tooling can render consistent diagnostics. `quantum_remaining` exposes the unused portion of the outgoing PID's configured quantum (clamped to zero once consumed), while `post_state` and `next_state` capture the scheduler view after the switch. `executed` reflects the instruction count reported by the VM for the triggering tick and `source` indicates whether the quantum advanced via the auto-runner or a manual `step`/`clock` invocation. Additional, tool-specific metadata may appear under `details` and must be ignored if unrecognised.

**Register Change Tracking:**
The `trace_step` event may include an optional `changed_regs` field listing only modified registers (e.g., `["R1", "R2", "PC"]`) to optimize TUI highlighting and reduce bandwidth.

**Task State Events:**
The `task_state` event explicitly tracks individual task state transitions with reason codes (`debug_break`, `sleep`, `mailbox_wait`, `user_pause`, etc.), enabling TUI status bar updates.

### 7.3 Back-pressure and ACKs
- Each session owns a bounded ring buffer (`max_events` negotiated during `session.open`; default 256). Overflow drops oldest entries and emits a `warning` event with `reason: "backpressure"`.
- Clients send `events.ack` with the highest processed sequence to free space eagerly; otherwise events expire after a configurable timeout (default 5 s).
- `events.unsubscribe` stops streaming and releases buffers automatically when sessions close.

### 7.4 Error Handling
- Invalid or unsupported categories return `status: "error"` with `error: "unsupported_category:<name>"`.
- Subscriptions without an active session return `session_required`.
- After transient failures clients reconnect with `session.open` and resume from the last processed sequence via `since_seq`.

### 7.5 Reconnect and Keepalive
- Session locks, keepalive heartbeats, and replay windows work together to resume streams predictably. Tooling maintains a replay window sized to the negotiated `max_events` while the executive drops inactive observers first to protect owner sessions.

## 8. Scheduler & Run Control
### 8.1 State Machine
```text
        +---------+        step()        +----------+
        |  READY  | --------------------> | RUNNING  |
        +----+----+                       +----+-----+
             ^                                 |
             |                                 | single instruction
             |                                 v
             |                        +------------------+
             +------------------------|  Post-instruction|
                                      +----+-------------+
                                           |
                                           | mailbox wait satisfied / yield
                                           v
                                         READY

                           no data, blocking
                         +-------------------+
                         |                   |
                         v                   |
                   +-----------+             |
                   | WAIT_MBX  |-------------+
                   +-----+-----+
                         |
            timeout      | wake (data / capacity)
                v        v
             TIMEOUT --> READY

RUNNING -- sleep_ms --> SLEEPING -- wake deadline --> READY
RUNNING -- debugger pause --> PAUSED -- resume/step --> READY
RUNNING -- task exit --> RETURNED
```

### 8.2 State Definitions
- **READY:** task eligible for selection in the round-robin queue (future priority overlays allowed).
- **RUNNING:** executive has called `vm_set_context(pid)` to switch to this task, then retired exactly one `MiniVM.step()`; post-instruction handling determines next state.
- **WAIT_MBX:** task blocked on mailbox send/receive; descriptor wait lists record PID and wake on mailbox or timeout.
- **SLEEPING:** task requested `sleep_ms`; timer heap tracks wake deadlines.
- **PAUSED:** debugger or shell suspended execution; auto-clock stops until resume.
- **RETURNED / TERMINATED:** terminal states after SVC exit or fatal error. Cleanup removes task from scheduling structures.
- **KILLED:** explicit termination path invoked via shell/debugger; transitions immediately to TERMINATED.

Python implementation exposes these states via a `TaskState` enum. All scheduler transitions are validated against the diagram above and recorded for debugging hooks (`state.last_state_transition`) so tooling can inspect why a PID moved between states.

**Context switching:** When transitioning from one READY task to another, the executive calls `vm_set_context(new_pid)`. The VM internally saves the current task's state (PC, SP, registers) and loads the new task's state. The executive never directly manipulates PC, SP, or register values—this maintains security boundaries and prevents executive code from corrupting task execution state.

### 8.3 Invariants
- Single-instruction quantum: each scheduler turn executes one instruction per active PID. `TaskContext.accounted_steps` tracks totals for diagnostics.
- Exclusive RUNNING: the executive never issues nested `step()` calls while a PID is active.
- Wait list integrity: a PID appears in at most one wait queue; wake and timeout paths remove entries before requeueing.
- Deterministic order: READY queue preserves PID order unless explicit priority overrides apply. Wake and timeout reinsertion occurs at the tail to avoid starvation.
- Debugger safety: transitioning to PAUSED flushes events and halts auto-clock threads before re-entering the VM.

### 8.4 Wait/Wake Integration
- Mailbox manager reports waiters and wake events. On wake the executive marks the PID READY, emits `mailbox_wake`, and replays pending payloads during the next mailbox receive call.
- Sleep timers use a monotonic min-heap keyed by deadline; scheduler polls before each round-robin iteration and moves expired entries to READY while emitting `task_wake`.
- External interrupts such as attach/kill requests update state outside the VM. If a task is RUNNING when a debugger pause arrives, the executive schedules the pause after the current instruction to preserve the single-instruction contract.
- Sleep syscalls are non-blocking inside the VM; the executive records the deadline in its timer heap and only schedules the PID once the deadline expires, preserving the single-instruction quantum.

### 8.5 Fairness & Accounting
- Round-robin order ensures each READY task receives a turn; tasks that immediately re-yield stay at the tail to avoid tight loops.
- Metrics (`scheduler_trace`, `scheduler_counters`) log transitions (READY->RUNNING, RUNNING->WAIT_MBX, etc.) so tooling can detect imbalance or starvation.
- Future priority support must maintain the single-instruction invariant and respect wait/wake semantics.
- Runtime instrumentation (for example `resource_usage` RPC) compares live descriptor and stack usage against the budgets in `docs/resource_budgets.md`; test plans include build-time (`avr-size`) and run-time (guard page) checks.

### 8.6 Breakpoint Management
- Maintain per-PID breakpoint sets.
- Pre-step gate: before calling `vm_step` or `vm_clock`, read `REG_PC`. If it matches a breakpoint, emit `debug_break` and skip execution.
- Post-step check: respect BRK opcodes by interpreting `StepResult.reason == break` and emitting `debug_break`.
- Breakpoints leave the PID in READY/STOPPED state until the user resumes or clears them.

### 8.7 Run Loop Pseudocode
```text
for pid in sched.ready():
    if debug_session and pid in bp_table and reg.get(pid, REG_PC) in bp_table[pid]:
        emit(debug_break(pid, pc))
        continue
    res = vm.clock(quantum)  # or vm.step()
    if res.reason == svc:
        handle_svc(res.svc_id, pid)
    elif res.reason == break:
        emit(debug_break(pid, res.pc))
    elif res.reason == fault:
        emit(fault(pid, res.pc))
        park(pid)
    sched.account(pid, res)
```

### 8.8 Event Protocol Integration
- Event categories and field definitions are governed by `docs/executive_protocol.md`. ACK, rate limiting, and back-pressure rules apply uniformly to `debug_break`, `fault`, `svc_error`, `sched_slice`, and related events.
- Reconnect semantics reuse the session lock, keepalive, and replay window defined in the event streaming contract.

## 9. Edge Cases
- Detect MiniVM disconnects or crashes, automatically restart if configured, and notify sessions.
- Enforce PID locks for concurrent clients; plan for read-only observer mode without breaking shell semantics.
- Handle resource exhaustion: descriptor or value table saturation, FRAM key collisions, event queue overflow (throttle or drop with diagnostics).
- Throttle clocks when all tasks block on mailbox waits to avoid busy loops.
- Current Python implementation snapshots mailbox waiters; after register-window remediation ensure `_store_active_state` stops cloning register banks.
- Provisioning failure (CRC mismatch, missing FRAM key) rolls back to the previous image and notifies the host via event stream.
- CAN-induced command storms must respect session locks and rate limits to prevent starvation.

## 10. Security & Access Control (Design Option)
- Current prototypes assume trusted shell/debugger clients on a lab network; no authentication or ACL checks exist on RPC sessions, provisioning channels, or value/command calls.
- Potential hooks:
  - Session-level capabilities negotiated during `session.open` (observer versus control, optional shared secrets).
  - Value/command descriptors extended with `auth_level` or token hints enforced before dispatch.
  - Provisioning manifest signatures layered atop existing `.hxe` CRC checks.
  - Rate-limited mailbox or CAN ingress to mitigate flooding.
- `docs/security.md` tracks this placeholder scope and will evolve once security goals are defined. Documentation must remain explicit about the lack of enforcement until the design option graduates.

## 11. Testing Strategy
- RPC integration via `python/tests/test_exec_smoke.py` plus CLI smoke scripts.
- Auto-clock and run-to-PC regression coverage (`python/tests/test_vm_pause.py`).
- Session lock conflict tests for simultaneous shell/debugger attaches.
- Provisioning and persistence simulations (mock CAN transfers, FRAM load/save unit tests).
- Post-remediation scheduler tests confirming fairness and non-copying context switches (ties to `issues/#2_scheduler` DoD).
- Event-stream regression tests verifying deterministic sequences and drop notifications for subscribers.
- Resource budget checks: integrate section-size assertions (`avr-size`) and runtime telemetry to verify stack/mailbox usage stays within documented limits.
- Security/ACL tests for command/value access once policy hooks exist.

## 12. Traceability
- Design Requirements: DR-1.1, DR-1.2, DR-1.3, DR-2.5, DR-3.1, DR-5.1, DR-5.2, DR-5.3, DR-6.1, DR-7.1, DR-8.1.
- Design Goals: DG-1.2, DG-1.3, DG-1.4, DG-2.3, DG-3.1..DG-3.5, DG-4.2, DG-5.1..DG-5.4, DG-6.1..DG-6.4, DG-7.1..DG-7.3, DG-8.1..DG-8.3.
- Design Options: DO-5.a, DO-6.a, DO-7.a, DO-relay.
